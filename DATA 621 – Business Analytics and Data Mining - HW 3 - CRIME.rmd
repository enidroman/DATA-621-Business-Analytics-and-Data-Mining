---
title: "DATA 621 – Business Analytics and Data Mining - HW 3 - CRIME"
author: "Enid Roman"
date: "2023-11-06"
output:
  word_document: default
  theme: default
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


```{r}
# load libraries
suppressWarnings({
  # Code that generates specific warnings
  # Other code
  library(tidyverse)
  library(psych)
  library(corrplot)
  library(RColorBrewer)
  library(knitr)
  library(MASS)
  library(caret)
  library(kableExtra)
  library(ResourceSelection)
  library(pROC)
})

suppressMessages({
  library(tidyverse)
  library(psych)
  library(corrplot)
  library(RColorBrewer)
  library(knitr)
  library(MASS)
  library(caret)
  library(kableExtra)
  library(ResourceSelection)
  library(pROC)
})

```


```{r}
#load data
crime_train <- read.csv("https://raw.githubusercontent.com/enidroman/DATA-621-Business-Analytics-and-Data-Mining/main/crime-training-data_modified.csv")
crime_test <- read.csv("https://raw.githubusercontent.com/enidroman/DATA-621-Business-Analytics-and-Data-Mining/main/crime-evaluation-data_modified.csv")
```


## <span style="color: blue;">**INTRODUCTION**</span>

In this assignment, I will explore, analyze and model a data set containing information on crime for various neighborhoods of a major city. Each record has a response variable indicating whether or not the crime rate is above the median crime rate (1) or not (0).

My objective is to build a binary logistic regression model on the training data set to predict whether the
neighborhood will be at risk for high crime levels. I will provide classifications and probabilities for the
evaluation data set using my binary logistic regression model. I can only use the variables given to me (or
variables that I derive from the variables provided). Below is a short description of the variables of interest in
the data set:


```{r}
vn <- c("zn", "indus", "chas", "nox", "rm", "age", "dis", "rad", "tax", "ptratio", "lstat", "medv", "target")
dscrptn <- c("proportion of residential land zoned for large lots (over 25000 square feet)", "proportion of non-retail business acres per suburb", "a dummy var. for whether the suburb borders the Charles River (1) or not (0)", "nitrogen oxides concentration (parts per 10 million)", "average number of rooms per dwelling", "proportion of owner-occupied units built prior to 1940", "weighted mean of distances to five Boston employment centers", "index of accessibility to radial highways", "full-value property-tax rate per $10,000", "pupil-teacher ratio by town","lower status of the population (percent)", "median value of owner-occupied homes in $1000s", "whether the crime rate is above the median crime rate (1) or not (0)")
kable(cbind(vn, dscrptn), col.names = c("Variable Name", "Short Description")) %>% 
  kable_styling(full_width = T)
```


## <span style="color: blue;">**DATA EXPLORATION**</span>

### <span style="color: blue;">**DATA SUMMARY**</span>


The dataset consists of 13 variables and 466 observations with no missing values. One of the variable chas, is a dummy variable and the rest are numerical variables. Additionally, describe function from psych package shows us the mean, standard deviation, skewness and other statistical analysis.


```{r}
# summary statistics
crime_train %>% 
  mutate(chas = as.factor(chas), 
         target = as.factor(target)) %>% 
  glimpse() %>% 
  describe()
```

### <span style="color: blue;">**MISSING VALUES**</span>

There are no missing values.


```{r}
# count the total number of missing values 
sum(is.na(crime_train))
```

### <span style="color: blue;">**OUTLIERS**</span>

In the box-plot, we observe many variables consists of outliers. There are also very high interquartile range for rad and tax variables where crime rate is above the median. Lastly, the variance between the 2 values of target differs for zn, nox, age, dis, rad & tax, which indicates that we will want to consider adding quadratic terms for them.


```{r}
# box-plot
crime_train %>%
  dplyr::select(-chas) %>% 
  gather(key, value, -target) %>% 
  mutate(key = factor(key),
         target = factor(target)) %>% 
  ggplot(aes(x = key, y = value)) +
  geom_boxplot(aes(fill = target)) +
  facet_wrap(~ key, scales = 'free', ncol = 3) +
  scale_fill_manual(values=c("#999999", "#E69F00")) +
  theme_minimal()
```

### <span style="color: blue;">**VARIABLE DISTRIBUTION**</span>

Here the medv, and rm are normally distributed. I also see bi-modal distribution of the variables indus, rad and tax. The rest of the variables show moderate to high skewness on either side respectively. Also, dis, medv, nox, rad, rm, tax, and zn have outliers.  


```{r}

crime_train %>%
  gather(key, value, -c(target, chas)) %>%
  ggplot(aes(value)) +
  geom_histogram(binwidth = function(x) 2 * IQR(x) / (length(x)^(1/3)), fill="#56B4E9") +
  facet_wrap(~ key, scales = 'free', ncol = 3) +
  theme_minimal()
```

### <span style="color: blue;">**CORRELATION**</span>

In both correlation table and plot below, there are moderate positive correlation between variables nox, age, rad, tax, indus and target variables; and moderate negative correlation between variable dis. The rest of the variables have weak or no correlations.

#### <span style="color: blue;">**CORRELATION TABLE**</span>


```{r}
kable(sort(cor(dplyr::select(crime_train, target, everything()))[,1], decreasing = T), col.names = c("Correlation")) %>% 
  kable_styling(full_width = F)
```


#### <span style="color: blue;">**CORRELATION PLOT**</span>


```{r}
crime_train %>% 
  cor(.) %>%
  corrplot(., method = "color", type = "upper", tl.col = "black", diag = FALSE)
```

## <span style="color: blue;">**DATA PREPARATION**</span>

### <span style="color: blue;">**MULTICOLLINEAR VARIABLES**</span>

In my visualization analysis, some of the variables are skewed, have outliers or follow a bi-modal distribution. I performed transformation on some of these variables. I removed the variable tax because of multicollinearity and it’s high VIF score. Then took log transformation of age and lstat variables to lower skewness. Lastly, I added quadratic term to zn, rad, and nox variables to account for its variances with respect to target variable.


```{r}
kable((car::vif(glm(target ~. , data = crime_train))), col.names = c("VIF Score")) %>%  #remove tax for high vif score
  kable_styling(full_width = F)
```


### <span style="color: blue;">**TRANSFORMATION OF VARIABLES**</span>


```{r}
 crime_train_trans <- crime_train %>%
  dplyr::select(-tax) %>% 
  mutate(age = log(age),
         lstat = log(lstat),
         zn = zn^2,
         rad = rad^2,
         nox = I(nox^2))
```


## <span style="color: blue;">**BUILD MODELS**</span>


I will build three different models to see which one yields the best performance. In the first model below, I used all original variables.  

### <span style="color: blue;">**FIRST MODEL - ALL ORIGINAL VARIABLES**

First model I used all original variables. 7 of the 12 variables have statistically significant p-values. In the Hosmer-Lemeshow goodness-of-fit test, the null hypothesis is rejected due to low p-value.


```{r}
# model 1 with all original variables
model1 <- glm(target ~ ., family = "binomial", crime_train)
summary(model1)
```

#### <span style="color: blue;">**GOODNESS OF FIT TEST**


```{r}
# goodness of fit test
hoslem.test(crime_train$target, fitted(model1))
```

### <span style="color: blue;">**SECOND MODEL - TRANSFORMED VARIABLES**

Second model I used our transformed variables. Our model yielded relatively same results compared to model 1. Moreover, the p-value is low again thus this model’s goodness of fit null hypothesis is rejected as well.


```{r}
# model 2 with transformed variables. 
model2 <- glm(target ~ ., family = "binomial", crime_train_trans)
summary(model2)
```

#### <span style="color: blue;">**GOODNESS OF FIT TEST**


```{r}
# goodness of fit test
hoslem.test(crime_train$target, fitted(model1))
```

Since the transformed variables yielded a model that performs worse than the model with original variables, I applied a box-cox transformation to all the variables to see if it performs better. As seen previously, most of the dataset has many skewed variables. When an attribute has a normal distribution but is shifted, this is called a skew. The distribution of an attribute can be shifted to reduce the skew and make it more normal The Box Cox transform can perform this operation (assumes all values are positive).

Even though this model took less Fisher Scoring iterations than other models, it too yielded similar results and low p-value as the other two models.


### <span style="color: blue;">**BOXCOX TRANSFORMATION**


```{r}
# boxcox transformation use caret package
crime_boxcox <- preProcess(crime_train, c("BoxCox"))
cb_transformed <- predict(crime_boxcox, crime_train)
model <- glm(target ~ ., family = "binomial", cb_transformed)
summary(model)
```

#### <span style="color: blue;">**GOODNESS OF FIT TEST**


```{r}
# goodness of fit test
hoslem.test(crime_train$target, fitted(model))
```

### <span style="color: blue;">**THIRD MODEL - STEPWISE SELECTION**

Third model, I useD the stepwise selection from the MASS package. This model yields the best performance so far. It has the lowest AIC Score and all of the variables have significant p-value. I select this model to make prediction.


```{r}
# model 3 stepwise selection of variables 
model3 <- stepAIC(model1, direction = "both", trace = FALSE)
summary(model3)
```

#### <span style="color: blue;">**GOODNESS OF FIT TEST**


```{r}
# goodness of fit test
hoslem.test(crime_train$target, fitted(model3))
```

## <span style="color: blue;">**SELECT MODEL**


### <span style="color: blue;">**MODEL COMPARISON**


I compared various metrics for all three models in order to make the prediction. I calculated all three models’ accuracy, classification error rate, precision, sensitivity, specificity, F1 score, AUC, and confusion matrix. 

Even though model 1 performs better in every metrics, the difference is very small. I will pick model 3 with stepwise variable selection because it has the lowest AIC score and all variables have high p-values.


```{r}
# comparing all models using various measures
c1 <- confusionMatrix(as.factor(as.integer(fitted(model1) > .5)), as.factor(model1$y), positive = "1")
c2 <- confusionMatrix(as.factor(as.integer(fitted(model2) > .5)), as.factor(model2$y), positive = "1")
c3 <- confusionMatrix(as.factor(as.integer(fitted(model3) > .5)), as.factor(model3$y), positive = "1")

roc1 <- roc(crime_train$target,  predict(model1, crime_train, interval = "prediction"))
roc2 <- roc(crime_train$target,  predict(model2, crime_train, interval = "prediction"))
roc3 <- roc(crime_train$target,  predict(model3, crime_train, interval = "prediction"))

metrics1 <- c(c1$overall[1], "Class. Error Rate" = 1 - as.numeric(c1$overall[1]), c1$byClass[c(1, 2, 5, 7)], AUC = roc1$auc)
metrics2 <- c(c2$overall[1], "Class. Error Rate" = 1 - as.numeric(c2$overall[1]), c2$byClass[c(1, 2, 5, 7)], AUC = roc2$auc)
metrics3 <- c(c3$overall[1], "Class. Error Rate" = 1 - as.numeric(c3$overall[1]), c3$byClass[c(1, 2, 5, 7)], AUC = roc3$auc)

kable(cbind(metrics1, metrics2, metrics3), col.names = c("Model 1", "Model 2", "Model 3"))  %>% 
  kable_styling(full_width = T)
```


### <span style="color: blue;">**ROC CURVE**


Since the ROC plots the true positive rates against the false postive rates, I am look for the area under the curve to be close to 1.

.972 is very close to 1, so this is a satisfactory model.


```{r}
# plotting roc curve of model 3
plot(roc(crime_train$target,  predict(model3, crime_train, interval = "prediction")), print.auc = TRUE)
```

### <span style="color: blue;">**PREDICTION**


```{r}
# prepare evaualtion dataset
crime_test <- crime_test %>% 
  mutate(chas = as.factor(chas))
```


```{r}
# prediction
predict <- predict(model3, crime_test, interval = "prediction")
eval <- table(as.integer(predict > .5))
print(paste(eval[1], "are above median crime rate", "and", eval[2], "are below median crime rate."))
```

```{r}
final_Pred =predict(model3, newdata=crime_test)
final_Pred = ifelse(final_Pred<.5,0,1)
hist(final_Pred)
```

## <span style="color: blue;">**CONCLUSION**

I explored explore, analyze and model a data set containing information on crime for various neighborhoods of a major city, which had no missing values but had some outliers. Some of the variables were normally distributed, while others were moderate to high skewness. There were positive correlation between variables and also moderate to negative correlations between variables. 

I prepared the data by transforming the variables to build the models. I built the first model with all the origional variables. The second model I built by transforming the variables by using boxcox transformation. I built the third model, which was the chosen one, by using teh stepwise selection method. 

In order to choose the prefered model I did a comparison of all three models. I finally did a ROC Curve for my chosen model 3, which resulted to a satisfactory model. 

Lastly, I did a prediction with the crime test dataset, which predicted 21 of the variables are above crime rate and 19 are below median crime rate. 


## <span style="color: blue;">**APPENDIX: ALL CODES**


```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}

```

